{
  "evaluations": {
    "1": {
      "generation": 1,
      "description": "Task Execution Complexity Scale - measures how difficult a prompt would be to execute via code, automation, or agentic workflows",
      "prompt_template": "EVALUATION CRITERIA (Generation {EVAL_GEN}):\nYou are a task complexity evaluation engine.\n\nFirst, internally define a **0–100 Task Execution Complexity Scale** for how difficult a prompt would be to execute via code, automation, or agentic workflows, where complexity reflects required planning depth, number of steps, branching logic, tool integrations, state management, uncertainty handling, data dependencies, and failure recovery requirements.\n\nThe scale must follow these anchors:\n\n* **0–10**: Single-step, no external tools, no state.\n* **11–30**: Few sequential steps, minimal logic, limited data handling.\n* **31–50**: Multi-step workflow with light branching and basic tool use.\n* **51–70**: Coordinated multi-system execution with moderate decision logic and validation.\n* **71–90**: Complex orchestration, dynamic decision-making, multiple integrations, error handling, and state tracking.\n* **91–100**: High-autonomy, adaptive, multi-phase execution requiring long-horizon planning, uncertainty resolution, and recovery mechanisms.\n\nThen, given the following input prompt, assign a **complexity score from 0–100** strictly based on execution requirements (not topic difficulty).\n\nEvaluation requirements:\n\n* Consider number of discrete executable steps.\n* Consider branching/conditional logic.\n* Consider required integrations or external systems.\n* Consider persistence of state across steps.\n* Consider need for validation or error handling.\n* Ignore writing difficulty or domain knowledge difficulty unless it impacts execution structure.\n\nOutput strictly in the following format:\n\nComplexity Score: <integer 0–100>\nJustification: <concise 3–6 sentence explanation referencing the scale>\n\nPrompt to evaluate:\n{INPUT_PROMPT}"
    }
  }
}
